{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a36bd2a-0beb-4ec4-ac2c-c76ae6cba4ef",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ace9fb7-c858-4d61-9a6b-636a92daa673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-26 02:01:46 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-26 02:01:46 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier_rag.readers\n",
    "from pyterrier_rag.backend import HuggingFaceBackend\n",
    "import pandas as pd\n",
    "\n",
    "import agent_framework\n",
    "from agent_framework import R1Searcher\n",
    "from agent_framework import SearchR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4d25de-e296-4de2-a606-57f4deb3c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started (triggered by _pt_tokeniser) and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "ds = pt.get_dataset('rag:nq') # or rag:hotpotqa\n",
    "TOPICS = ds.get_topics('dev').head(100)\n",
    "QRELS  = ds.get_answers('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a7f3e8-b710-42be-ac22-5167da9d0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_index = pt.Artifact.from_hf('pyterrier/ragwiki-terrier')\n",
    "bm25_ret = pt.rewrite.tokenise() >> sparse_index.bm25(include_fields=['docno', 'text', 'title'], threads=5, verbose = True) >> pt.rewrite.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aee0c6c2-a60a-46f7-a5be-b15fe044bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAS = [\n",
    "    pyterrier_rag.measures.F1,\n",
    "    pyterrier_rag.measures.EM,\n",
    "    # \"mrt\",\n",
    "    # \"llm_time\",\n",
    "    # \"retrieval_time\",\n",
    "    # \"turns\",\n",
    "    # \"gpu_usage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49d097-a557-4ebe-86dd-e240f31b3600",
   "metadata": {},
   "source": [
    "# RQ1--LLM calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea95cd9-e449-4ab2-86e1-f9acac156448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c933db361354bd3aa55020ea867b06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] transformers backend ready: XXsongLALA/Qwen-2.5-7B-base-RAG-RL on cuda:0\n",
      "INFO 08-26 02:22:27 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 08-26 02:22:27 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-26 02:22:27 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 08-26 02:22:28 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 08-26 02:22:33 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1ca04b8280>\n",
      "WARNING 08-26 02:22:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.30it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.46it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 08-26 02:22:37 [core.py:396] EngineCore failed to start.\n",
      "ERROR 08-26 02:22:37 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 08-26 02:22:37 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 08-26 02:22:37 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "ERROR 08-26 02:22:37 [core.py:396]     self._initialize_kv_caches(vllm_config)\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n",
      "ERROR 08-26 02:22:37 [core.py:396]     kv_cache_configs = [\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 134, in <listcomp>\n",
      "ERROR 08-26 02:22:37 [core.py:396]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\", line 699, in get_kv_cache_config\n",
      "ERROR 08-26 02:22:37 [core.py:396]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\n",
      "ERROR 08-26 02:22:37 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\", line 527, in check_enough_kv_cache_memory\n",
      "ERROR 08-26 02:22:37 [core.py:396]     raise ValueError(\"No available memory for the cache blocks. \"\n",
      "ERROR 08-26 02:22:37 [core.py:396] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n",
      "    kv_cache_configs = [\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 134, in <listcomp>\n",
      "    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\", line 699, in get_kv_cache_config\n",
      "    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py\", line 527, in check_enough_kv_cache_memory\n",
      "    raise ValueError(\"No available memory for the cache blocks. \"\n",
      "ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "[rank0]:[W826 02:22:37.287618990 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] vLLM init failed, fallback to transformers: Engine core initialization failed. See root cause above.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e5ede7abad40398ab8ff5b37658726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] transformers backend ready: XXsongLALA/Qwen-2.5-7B-base-RAG-RL on cuda:0\n",
      "INFO 08-26 02:22:40 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 08-26 02:22:40 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-26 02:22:40 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 08-26 02:22:45 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff9425baa10>\n",
      "WARNING 08-26 02:22:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "ERROR 08-26 02:22:46 [core.py:396] EngineCore failed to start.\n",
      "ERROR 08-26 02:22:46 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 08-26 02:22:46 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self._init_executor()\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.collective_rpc(\"load_model\")\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 08-26 02:22:46 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 08-26 02:22:46 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.model_runner.load_model()\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 08-26 02:22:46 [core.py:396]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 08-26 02:22:46 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 08-26 02:22:46 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 436, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 305, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
      "ERROR 08-26 02:22:46 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
      "ERROR 08-26 02:22:46 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 307, in <lambda>\n",
      "ERROR 08-26 02:22:46 [core.py:396]     lambda prefix: decoder_layer_type(config=config,\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 217, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.mlp = Qwen2MLP(\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 74, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     super().__init__(input_size=input_size,\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     self.quant_method.create_weights(\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 08-26 02:22:46 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 08-26 02:22:46 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 08-26 02:22:46 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 08-26 02:22:46 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 31.61 GiB of which 92.44 MiB is free. Process 2420245 has 28.67 GiB memory in use. Process 2452417 has 2.83 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 20.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "    model = _initialize_model(vllm_config=vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 436, in __init__\n",
      "    self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 305, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
      "    [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 307, in <lambda>\n",
      "    lambda prefix: decoder_layer_type(config=config,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 217, in __init__\n",
      "    self.mlp = Qwen2MLP(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 74, in __init__\n",
      "    self.gate_up_proj = MergedColumnParallelLinear(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "    super().__init__(input_size=input_size,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 31.61 GiB of which 92.44 MiB is free. Process 2420245 has 28.67 GiB memory in use. Process 2452417 has 2.83 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 20.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W826 02:22:46.368218145 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] vLLM init failed, fallback to transformers: Engine core initialization failed. See root cause above.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6104410357644fe8a5dcae0a7b735637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] transformers backend ready: XXsongLALA/Qwen-2.5-7B-base-RAG-RL on cuda:0\n",
      "INFO 08-26 02:22:49 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 08-26 02:22:49 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-26 02:22:49 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 08-26 02:22:54 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f89afe8da50>\n",
      "WARNING 08-26 02:22:54 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "ERROR 08-26 02:22:55 [core.py:396] EngineCore failed to start.\n",
      "ERROR 08-26 02:22:55 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 08-26 02:22:55 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self._init_executor()\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.collective_rpc(\"load_model\")\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 08-26 02:22:55 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 08-26 02:22:55 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.model_runner.load_model()\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 08-26 02:22:55 [core.py:396]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 08-26 02:22:55 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 08-26 02:22:55 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 436, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 305, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
      "ERROR 08-26 02:22:55 [core.py:396]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
      "ERROR 08-26 02:22:55 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 307, in <lambda>\n",
      "ERROR 08-26 02:22:55 [core.py:396]     lambda prefix: decoder_layer_type(config=config,\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 217, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.mlp = Qwen2MLP(\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 74, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     super().__init__(input_size=input_size,\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     self.quant_method.create_weights(\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 08-26 02:22:55 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 08-26 02:22:55 [core.py:396]   File \"/opt/miniconda3/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 08-26 02:22:55 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 08-26 02:22:55 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 31.61 GiB of which 26.44 MiB is free. Process 2420245 has 30.12 GiB memory in use. Process 2452757 has 1.45 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 27.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "    model = _initialize_model(vllm_config=vllm_config)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 436, in __init__\n",
      "    self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 305, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
      "    [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 307, in <lambda>\n",
      "    lambda prefix: decoder_layer_type(config=config,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 217, in __init__\n",
      "    self.mlp = Qwen2MLP(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 74, in __init__\n",
      "    self.gate_up_proj = MergedColumnParallelLinear(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "    super().__init__(input_size=input_size,\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 31.61 GiB of which 26.44 MiB is free. Process 2420245 has 30.12 GiB memory in use. Process 2452757 has 1.45 GiB memory in use. Of the allocated memory 1.07 GiB is allocated by PyTorch, and 27.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W826 02:22:55.963550567 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] vLLM init failed, fallback to transformers: Engine core initialization failed. See root cause above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb400fa9ecd446c1a2c2ba4be8c946ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] transformers backend ready: Qwen/Qwen2.5-3B-Instruct on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ---- 四个系统变体 ----\n",
    "hf_7b = R1Searcher(\n",
    "    retriever=bm25_ret,\n",
    "    model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "    use_vllm=False,                 # 强制 HF 顺序生成\n",
    "    max_turn=6, top_k=8, max_tokens=512, verbose=True, prompt_type='v1'\n",
    ")\n",
    "\n",
    "vllm_7b = R1Searcher(\n",
    "    retriever=bm25_ret,\n",
    "    model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "    use_vllm=True,                  # vLLM 并行生成\n",
    "    max_turn=6, top_k=8, max_tokens=512, verbose=True, prompt_type='v1'\n",
    ")\n",
    "\n",
    "vllm_7b_smallGPU = R1Searcher(\n",
    "    retriever=bm25_ret,\n",
    "    model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "    use_vllm=True,\n",
    "    model_kw_args=dict(             # 更保守的显存/上下文配置\n",
    "        gpu_memory_utilization=0.60,\n",
    "        max_model_len=768,\n",
    "        tensor_parallel_size=1\n",
    "    ),\n",
    "    max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    ")\n",
    "\n",
    "vllm_3b = R1Searcher(\n",
    "    retriever=bm25_ret,\n",
    "    model_id=\"Qwen/Qwen2.5-3B-Instruct\",   # 可换成你本地已可用的小模型\n",
    "    use_vllm=True,\n",
    "    max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e72e537-c498-4acd-b0a8-73edbedcc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/52 [00:00<?, ?batches/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "pt.Experiment:   0%|          | 0/52 [00:00<?, ?batches/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Cdesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rq1_df \u001b[38;5;241m=\u001b[39m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhf_7b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_7b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_7b_smallGPU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_3b\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOPICS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQRELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMEAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF-7B (sequential)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvLLM-7B (parallel)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvLLM-7B (small-GPU)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvLLM-3B (parallel)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m rq1_df\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/pipelines.py:645\u001b[0m, in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, save_format, precompute_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognised save_mode \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(save_format)) \n\u001b[1;32m    643\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, save_ext))\n\u001b[0;32m--> 645\u001b[0m time, evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_run_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_topic_qids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m     evalDictsPerQ\u001b[38;5;241m.\u001b[39mappend(evalMeasuresDict)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/pipelines.py:393\u001b[0m, in \u001b[0;36m_run_and_evaluate\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, save_format, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     batch_topics : pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (res, batch_topics) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m( system\u001b[38;5;241m.\u001b[39mtransform_gen(topics, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, output_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m topics, but no results received in batch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(batch_topics), i, \u001b[38;5;28mstr\u001b[39m(system) ) )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/transformer.py:170\u001b[0m, in \u001b[0;36mTransformer.transform_gen\u001b[0;34m(self, input, batch_size, output_topics)\u001b[0m\n\u001b[1;32m    168\u001b[0m batch_topics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(batch)\n\u001b[1;32m    169\u001b[0m batch\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m--> 170\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_topics:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m res, batch_topics\n",
      "File \u001b[0;32m/mnt/primary/pyterrier/tests/agent_framework.py:113\u001b[0m, in \u001b[0;36mAgenticRAG.transform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# 1. 批量调用LLM生成\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#1. call the LLM for each query still active\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs : List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate_active_queries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# outputs: List[str]\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#2. check for answer in each of the :\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# if we see the question has been answered:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# extract the answer\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# remove this query from state_active, add to state_finished list\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 2. 检查每个输出是否已经有答案\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# 这里的batch_answers是使用check_answers方法提取出的列表，带有序号\u001b[39;00m\n\u001b[1;32m    120\u001b[0m batch_answers : List[\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m|\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_answers(outputs)  \u001b[38;5;66;03m# List[answer or None]\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/primary/pyterrier/tests/agent_framework.py:668\u001b[0m, in \u001b[0;36mR1Searcher.generate\u001b[0;34m(self, contexts)\u001b[0m\n\u001b[1;32m    665\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 668\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;66;03m# 仅解码新生成段，按每条样本原始长度裁切\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:3431\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:823\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    819\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    820\u001b[0m )\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 823\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:549\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    538\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    539\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         position_embeddings,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:262\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:164\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    162\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 164\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    165\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Cdesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`"
     ]
    }
   ],
   "source": [
    "rq1_df = pt.Experiment(\n",
    "    [hf_7b, vllm_7b, vllm_7b_smallGPU, vllm_3b],\n",
    "    TOPICS, QRELS, MEAS,\n",
    "    names=[\n",
    "        \"HF-7B (sequential)\",\n",
    "        \"vLLM-7B (parallel)\",\n",
    "        \"vLLM-7B (small-GPU)\",\n",
    "        \"vLLM-3B (parallel)\"\n",
    "    ],\n",
    "    batch_size=8,\n",
    "    verbose=True\n",
    ")\n",
    "rq1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53a284-e2f1-4fe8-abb2-741ec41d62a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
