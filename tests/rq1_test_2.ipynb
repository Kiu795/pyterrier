{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58383a5c-4058-42cf-9482-1b90152f6cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-28 23:06:45 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-28 23:06:45 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started (triggered by _pt_tokeniser) and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:219: UserWarning: Multi-threaded retrieval is experimental, YMMV.\n",
      "  warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:226: UserWarning: Upgrading indexref /mnt/resources/pyterrier-cache/artifacts/ebfd80cc597a31719f11ab5cd11ad8f441bc460f760c82ff66413ba9fb06943f/data.properties to be concurrent\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier_rag.readers\n",
    "from pyterrier_rag.backend import HuggingFaceBackend\n",
    "import pandas as pd\n",
    "import torch, time, os\n",
    "import agent_framework\n",
    "from agent_framework import R1Searcher\n",
    "from agent_framework import SearchR1\n",
    "\n",
    "ds = pt.get_dataset('rag:nq') # or rag:hotpotqa\n",
    "TOPICS = ds.get_topics('dev').head(100)\n",
    "QRELS  = ds.get_answers('dev')\n",
    "\n",
    "sparse_index = pt.Artifact.from_hf('pyterrier/ragwiki-terrier')\n",
    "bm25_ret = pt.rewrite.tokenise() >> sparse_index.bm25(include_fields=['docno', 'text', 'title'], threads=5, verbose = True) >> pt.rewrite.reset()\n",
    "\n",
    "MEAS = [\n",
    "    pyterrier_rag.measures.F1,\n",
    "    pyterrier_rag.measures.EM,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d93e71f-0b53-4cb6-9231-c3e50bb0e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f1_em(name, system, batch_size=8):\n",
    "    df = pt.Experiment(\n",
    "        [system], TOPICS, QRELS, MEAS,\n",
    "        names=[name], batch_size=batch_size, verbose=True\n",
    "    )\n",
    "    out_path = f\"f1em_{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_path\n",
    "    \n",
    "def get_gpu_mem(device=0):\n",
    "    \"\"\"返回 GPU 占用 (GB)，HF 和 vLLM 通用\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\", stdout=subprocess.PIPE\n",
    "        )\n",
    "        mems = [float(x) for x in result.stdout.strip().split(\"\\n\")]\n",
    "        return mems[device] / 1024\n",
    "    except Exception as e:\n",
    "        print(\"GPU usage read error:\", e)\n",
    "        return 0.0\n",
    "        \n",
    "def run_efficiency(name, system):\n",
    "    # 端到端时间 + 迭代次数 + GPU峰值\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t0 = time.time()\n",
    "    out = system.transform(TOPICS.copy())\n",
    "    total = time.time() - t0\n",
    "    mrt = total / len(TOPICS)\n",
    "\n",
    "    turns = out.get('search_iterations', pd.Series([0]*len(out))).astype(int).mean()\n",
    "    gpu_gb = get_gpu_mem()\n",
    "\n",
    "    rec = {'name': name, 'MRT': mrt, 'Turns': turns, 'GPU_GB': gpu_gb}\n",
    "    df = pd.DataFrame([rec])\n",
    "    out_path = f\"eff_{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f7f93c-0ab7-48f2-9fb5-95bfed6b07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset =  pt.get_dataset('rag:nq')\n",
    "# from ir_measures import define_byquery\n",
    "# Iterations = define_byquery(lambda qrels, run: run.iloc[0].iteration, name=\"Iterations\")\n",
    "# pt.Experiment(\n",
    "#     [safe_vllm_7b],\n",
    "#     dataset.get_topics('dev').head(100), # NB: remove .head(100) to run on all dev topics\n",
    "#     dataset.get_answers('dev'),\n",
    "#     [pyterrier_rag.measures.F1, pyterrier_rag.measures.EM, \"mrt\"],\n",
    "#     batch_size=8,\n",
    "#     verbose=True,\n",
    "#     names=['vLLM-7B']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163d0b83-2c55-4126-b18c-c63d2d4565a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-28 23:07:17 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 08-28 23:07:17 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-28 23:07:17 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-28 23:07:20 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='XXsongLALA/Qwen-2.5-7B-base-RAG-RL', speculative_config=None, tokenizer='XXsongLALA/Qwen-2.5-7B-base-RAG-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=5096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=XXsongLALA/Qwen-2.5-7B-base-RAG-RL, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 08-28 23:07:20 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f75171c3790>\n",
      "INFO 08-28 23:07:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-28 23:07:20 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 08-28 23:07:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-28 23:07:20 [gpu_model_runner.py:1329] Starting to load model XXsongLALA/Qwen-2.5-7B-base-RAG-RL...\n",
      "INFO 08-28 23:07:21 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d5818998b147b08032610541f6eb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-28 23:07:24 [loader.py:458] Loading weights took 2.62 seconds\n",
      "INFO 08-28 23:07:24 [gpu_model_runner.py:1347] Model loading took 14.2717 GiB and 3.457278 seconds\n",
      "INFO 08-28 23:07:26 [kv_cache_utils.py:634] GPU KV cache size: 120,096 tokens\n",
      "INFO 08-28 23:07:26 [kv_cache_utils.py:637] Maximum concurrency for 5,096 tokens per request: 23.57x\n",
      "INFO 08-28 23:07:26 [core.py:159] init engine (profile, create kv cache, warmup model) took 2.36 seconds\n",
      "INFO 08-28 23:07:26 [core_client.py:439] Core engine process 0 ready.\n",
      "[R1Searcher] vLLM backend ready: XXsongLALA/Qwen-2.5-7B-base-RAG-RL\n"
     ]
    }
   ],
   "source": [
    "# 1) vLLM-7B (parallel)\n",
    "vllm_7b = R1Searcher(\n",
    "    retriever=bm25_ret,\n",
    ")\n",
    "# # 2) HF-7B (sequential) need changes\n",
    "# vllm_7b = R1Searcher(retriever=bm25_ret,\n",
    "#     model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "#     use_vllm=True, max_turn=6, top_k=8, max_tokens=512, verbose=True, prompt_type='v1'\n",
    "# )\n",
    "# run_f1_em(\"vLLM-7B (parallel)\", vllm_7b, batch_size=8)\n",
    "# run_efficiency(\"vLLM-7B (parallel)\", vllm_7b)\n",
    "# del vllm_7b; torch.cuda.empty_cache()\n",
    "\n",
    "# # 3) vLLM-7B (small-GPU)\n",
    "# vllm_7b_small = R1Searcher(retriever=bm25_ret,\n",
    "#     model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "#     use_vllm=True, model_kw_args=dict(gpu_memory_utilization=0.60, max_model_len=768, tensor_parallel_size=1),\n",
    "#     max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    "# )\n",
    "# run_f1_em(\"vLLM-7B (small-GPU)\", vllm_7b_small, batch_size=8)\n",
    "# run_efficiency(\"vLLM-7B (small-GPU)\", vllm_7b_small)\n",
    "# del vllm_7b_small; torch.cuda.empty_cache()\n",
    "\n",
    "# # 4) Tiny model (可选)\n",
    "# tiny = R1Searcher(retriever=bm25_ret,\n",
    "#     model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     use_vllm=True, max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    "# )\n",
    "# run_f1_em(\"TinyLlama-1.1B (parallel)\", tiny, batch_size=8)\n",
    "# run_efficiency(\"TinyLlama-1.1B (parallel)\", tiny)\n",
    "# del tiny; torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d794307a-433d-4bfd-92ea-74fe7cceeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_qanswer_str(df):\n",
    "    # 没有该列，新建；有 None，用空串代替\n",
    "    if 'qanswer' not in df.columns:\n",
    "        df['qanswer'] = \"\"\n",
    "    else:\n",
    "        df['qanswer'] = df['qanswer'].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "# 正确：使用 pt.apply.generic 构造一个 transformer\n",
    "safe_vllm_7b = vllm_7b >> pt.apply.generic(ensure_qanswer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714a1840-3130-4dbb-a16d-64f296fdc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/13 [00:00<?, ?batches/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00715ce8728c44cca487a53fca055c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:02<00:16,  2.37s/q]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:02<00:06,  1.12s/q]\u001b[A\n",
      "TerrierRetr(BM25):  50%|█████     | 4/8 [00:02<00:01,  2.01q/s]\u001b[A\n",
      "TerrierRetr(BM25):  75%|███████▌  | 6/8 [00:03<00:00,  3.32q/s]\u001b[A\n",
      "TerrierRetr(BM25):  88%|████████▊ | 7/8 [00:03<00:00,  3.19q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:04<00:00,  1.89q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cea00c043241d0a3689a3478af7871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/2 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  50%|█████     | 1/2 [00:00<00:00,  5.40q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 2/2 [00:00<00:00,  6.98q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8280d748ed459d8e3c3c62f67ddb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   8%|▊         | 1/13 [00:12<02:30, 12.57s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad64a289d39047e6a8432a8bbb1b7034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:00<00:01,  4.81q/s]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:00<00:01,  5.38q/s]\u001b[A\n",
      "TerrierRetr(BM25):  50%|█████     | 4/8 [00:00<00:00,  7.81q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:00<00:00, 11.64q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c378650f11db4036aeddbd598e50f6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/1 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 1/1 [00:00<00:00,  8.70q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b880c7ddbe434b90b3acd53af8cda747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  15%|█▌        | 2/13 [00:21<01:56, 10.56s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7185bc363cdc44a4b37191559a371d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:00<00:01,  5.84q/s]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:00<00:00,  6.28q/s]\u001b[A\n",
      "TerrierRetr(BM25):  75%|███████▌  | 6/8 [00:00<00:00, 13.39q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:01<00:00,  7.34q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c2836a3aa641a4bbeb2a0f21443863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/2 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 2/2 [00:00<00:00,  4.76q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8716e423170647478699efb4fa7bcd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  23%|██▎       | 3/13 [00:31<01:41, 10.13s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7c12972fd7458cb902dca8d733bb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:00<00:02,  2.35q/s]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:00<00:01,  4.17q/s]\u001b[A\n",
      "TerrierRetr(BM25):  38%|███▊      | 3/8 [00:00<00:00,  5.49q/s]\u001b[A\n",
      "TerrierRetr(BM25):  62%|██████▎   | 5/8 [00:00<00:00,  7.41q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:00<00:00,  8.21q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435aa5b6730f4c6aa8d6079b40ffab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/2 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 2/2 [00:00<00:00,  8.48q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb934973f574fee8a560775fdcffbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  31%|███       | 4/13 [00:42<01:34, 10.51s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4507b61060a417aa93b49ce05b81712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:00<00:03,  2.28q/s]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:00<00:01,  4.11q/s]\u001b[A\n",
      "TerrierRetr(BM25):  75%|███████▌  | 6/8 [00:00<00:00,  9.88q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:00<00:00,  8.10q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143fe583affd4864adc81d28a4ae807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  38%|███▊      | 5/13 [00:50<01:17,  9.71s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1933e442e88434d815a3c40a45cd46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  12%|█▎        | 1/8 [00:00<00:06,  1.09q/s]\u001b[A\n",
      "TerrierRetr(BM25):  50%|█████     | 4/8 [00:01<00:00,  4.58q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:01<00:00,  5.92q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436e519770654cc49c74921a729cdb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/3 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  33%|███▎      | 1/3 [00:00<00:00,  4.08q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 3/3 [00:00<00:00,  7.42q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87deb7d09f0842b49eb551f85d87300b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  46%|████▌     | 6/13 [01:00<01:07,  9.70s/batches]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23bccefdb394447b94dc41b76cfefb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/8 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  25%|██▌       | 2/8 [00:00<00:00, 13.56q/s]\u001b[A\n",
      "TerrierRetr(BM25):  50%|█████     | 4/8 [00:00<00:00, 12.07q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 8/8 [00:00<00:00, 11.89q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981b9149108d4319be109a25563adf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrierRetr(BM25):   0%|          | 0/3 [00:00<?, ?q/s]\u001b[A\n",
      "TerrierRetr(BM25):  33%|███▎      | 1/3 [00:00<00:00,  5.61q/s]\u001b[A\n",
      "TerrierRetr(BM25): 100%|██████████| 3/3 [00:01<00:00,  2.35q/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeb6db7d5f14eeb8d24efbe8cbea02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  46%|████▌     | 6/13 [01:14<01:26, 12.34s/batches]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_f1_em\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvLLM-7B (parallel)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_vllm_7b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m run_efficiency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM-7B (parallel)\u001b[39m\u001b[38;5;124m\"\u001b[39m, safe_vllm_7b)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mrun_f1_em\u001b[0;34m(name, system, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_f1_em\u001b[39m(name, system, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOPICS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQRELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMEAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1em_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(out_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/pipelines.py:645\u001b[0m, in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, save_format, precompute_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognised save_mode \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(save_format)) \n\u001b[1;32m    643\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, save_ext))\n\u001b[0;32m--> 645\u001b[0m time, evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_run_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_topic_qids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m     evalDictsPerQ\u001b[38;5;241m.\u001b[39mappend(evalMeasuresDict)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/pipelines.py:393\u001b[0m, in \u001b[0;36m_run_and_evaluate\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, save_format, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     batch_topics : pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (res, batch_topics) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m( system\u001b[38;5;241m.\u001b[39mtransform_gen(topics, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, output_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m topics, but no results received in batch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(batch_topics), i, \u001b[38;5;28mstr\u001b[39m(system) ) )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/transformer.py:170\u001b[0m, in \u001b[0;36mTransformer.transform_gen\u001b[0;34m(self, input, batch_size, output_topics)\u001b[0m\n\u001b[1;32m    168\u001b[0m batch_topics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(batch)\n\u001b[1;32m    169\u001b[0m batch\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m--> 170\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_topics:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m res, batch_topics\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/pyterrier/_ops.py:372\u001b[0m, in \u001b[0;36mCompose.transform\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    370\u001b[0m out \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformers:\n\u001b[0;32m--> 372\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/primary/pyterrier/tests/agent_framework.py:113\u001b[0m, in \u001b[0;36mAgenticRAG.transform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# 1. 批量调用LLM生成\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#1. call the LLM for each query still active\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs : List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate_active_queries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# outputs: List[str]\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#2. check for answer in each of the :\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# if we see the question has been answered:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# extract the answer\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# remove this query from state_active, add to state_finished list\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 2. 检查每个输出是否已经有答案\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# 这里的batch_answers是使用check_answers方法提取出的列表，带有序号\u001b[39;00m\n\u001b[1;32m    120\u001b[0m batch_answers : List[\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m|\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_answers(outputs)  \u001b[38;5;66;03m# List[answer or None]\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/primary/pyterrier/tests/agent_framework.py:632\u001b[0m, in \u001b[0;36mR1Searcher.generate\u001b[0;34m(self, contexts)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, contexts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# vLLM 后端：原生支持批量\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 632\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m         texts: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/vllm/utils.py:1196\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1191\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1192\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1193\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m         )\n\u001b[0;32m-> 1196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:473\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    463\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    466\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    467\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    471\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 473\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1423\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1421\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1423\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:218\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m processed_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mprocess_outputs(\n\u001b[1;32m    222\u001b[0m     outputs\u001b[38;5;241m.\u001b[39moutputs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:556\u001b[0m, in \u001b[0;36mSyncMPClient.get_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EngineCoreOutputs:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_f1_em(\"vLLM-7B (parallel)\", safe_vllm_7b, batch_size=8)\n",
    "run_efficiency(\"vLLM-7B (parallel)\", safe_vllm_7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc935f-7ef3-4d18-bddd-833cc30f88d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
