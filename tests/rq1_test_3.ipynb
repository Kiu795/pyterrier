{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58383a5c-4058-42cf-9482-1b90152f6cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-28 22:56:52 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-28 22:56:52 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started (triggered by _pt_tokeniser) and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:219: UserWarning: Multi-threaded retrieval is experimental, YMMV.\n",
      "  warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:226: UserWarning: Upgrading indexref /mnt/resources/pyterrier-cache/artifacts/ebfd80cc597a31719f11ab5cd11ad8f441bc460f760c82ff66413ba9fb06943f/data.properties to be concurrent\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier_rag.readers\n",
    "from pyterrier_rag.backend import HuggingFaceBackend\n",
    "import pandas as pd\n",
    "import torch, time, os\n",
    "import agent_framework\n",
    "from agent_framework import R1Searcher\n",
    "from agent_framework import SearchR1\n",
    "\n",
    "ds = pt.get_dataset('rag:nq') # or rag:hotpotqa\n",
    "TOPICS = ds.get_topics('dev').head(100)\n",
    "QRELS  = ds.get_answers('dev')\n",
    "\n",
    "sparse_index = pt.Artifact.from_hf('pyterrier/ragwiki-terrier')\n",
    "bm25_ret = pt.rewrite.tokenise() >> sparse_index.bm25(include_fields=['docno', 'text', 'title'], threads=5, verbose = True) >> pt.rewrite.reset()\n",
    "\n",
    "MEAS = [\n",
    "    pyterrier_rag.measures.F1,\n",
    "    pyterrier_rag.measures.EM,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d93e71f-0b53-4cb6-9231-c3e50bb0e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f1_em(name, system, batch_size=8):\n",
    "    df = pt.Experiment(\n",
    "        [system], TOPICS, QRELS, MEAS,\n",
    "        names=[name], batch_size=batch_size, verbose=True\n",
    "    )\n",
    "    out_path = f\"f1em_{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_path\n",
    "    \n",
    "def get_gpu_mem(device=0):\n",
    "    \"\"\"返回 GPU 占用 (GB)，HF 和 vLLM 通用\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\", stdout=subprocess.PIPE\n",
    "        )\n",
    "        mems = [float(x) for x in result.stdout.strip().split(\"\\n\")]\n",
    "        return mems[device] / 1024\n",
    "    except Exception as e:\n",
    "        print(\"GPU usage read error:\", e)\n",
    "        return 0.0\n",
    "        \n",
    "def run_efficiency(name, system):\n",
    "    # 端到端时间 + 迭代次数 + GPU峰值\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t0 = time.time()\n",
    "    out = system.transform(TOPICS.copy())\n",
    "    total = time.time() - t0\n",
    "    mrt = total / len(TOPICS)\n",
    "\n",
    "    turns = out.get('search_iterations', pd.Series([0]*len(out))).astype(int).mean()\n",
    "    gpu_gb = get_gpu_mem()\n",
    "\n",
    "    rec = {'name': name, 'MRT': mrt, 'Turns': turns, 'GPU_GB': gpu_gb}\n",
    "    df = pd.DataFrame([rec])\n",
    "    out_path = f\"eff_{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f7f93c-0ab7-48f2-9fb5-95bfed6b07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset =  pt.get_dataset('rag:nq')\n",
    "# from ir_measures import define_byquery\n",
    "# Iterations = define_byquery(lambda qrels, run: run.iloc[0].iteration, name=\"Iterations\")\n",
    "# pt.Experiment(\n",
    "#     [safe_vllm_7b],\n",
    "#     dataset.get_topics('dev').head(100), # NB: remove .head(100) to run on all dev topics\n",
    "#     dataset.get_answers('dev'),\n",
    "#     [pyterrier_rag.measures.F1, pyterrier_rag.measures.EM, \"mrt\"],\n",
    "#     batch_size=8,\n",
    "#     verbose=True,\n",
    "#     names=['vLLM-7B']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd388d3-cd91-4c8b-804b-a9206d9ed23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d83ca366fc4c38ba1cfc79aa36364c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) vLLM-7B (parallel)\n",
    "# vllm_7b = R1Searcher(\n",
    "#     retriever=bm25_ret,\n",
    "# )\n",
    "# # 2) HF-7B (sequential) need changes\n",
    "hf_7b = R1Searcher(\n",
    "    retriever = bm25_ret,\n",
    "    use_vllm = False\n",
    ")\n",
    "\n",
    "# # 3) vLLM-7B (small-GPU)\n",
    "# vllm_7b_small = R1Searcher(retriever=bm25_ret,\n",
    "#     model_id=\"XXsongLALA/Qwen-2.5-7B-base-RAG-RL\",\n",
    "#     use_vllm=True, model_kw_args=dict(gpu_memory_utilization=0.60, max_model_len=768, tensor_parallel_size=1),\n",
    "#     max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    "# )\n",
    "# run_f1_em(\"vLLM-7B (small-GPU)\", vllm_7b_small, batch_size=8)\n",
    "# run_efficiency(\"vLLM-7B (small-GPU)\", vllm_7b_small)\n",
    "# del vllm_7b_small; torch.cuda.empty_cache()\n",
    "\n",
    "# # 4) Tiny model (可选)\n",
    "# tiny = R1Searcher(retriever=bm25_ret,\n",
    "#     model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     use_vllm=True, max_turn=6, top_k=8, max_tokens=384, verbose=True, prompt_type='v1'\n",
    "# )\n",
    "# run_f1_em(\"TinyLlama-1.1B (parallel)\", tiny, batch_size=8)\n",
    "# run_efficiency(\"TinyLlama-1.1B (parallel)\", tiny)\n",
    "# del tiny; torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794307a-433d-4bfd-92ea-74fe7cceeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_qanswer_str(df):\n",
    "    # 没有该列，新建；有 None，用空串代替\n",
    "    if 'qanswer' not in df.columns:\n",
    "        df['qanswer'] = \"\"\n",
    "    else:\n",
    "        df['qanswer'] = df['qanswer'].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "# 正确：使用 pt.apply.generic 构造一个 transformer\n",
    "hf_7b = hf_7b >> pt.apply.generic(ensure_qanswer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a1840-3130-4dbb-a16d-64f296fdc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_f1_em(\"HF-7B (sequential)\", hf_7b, batch_size=8)\n",
    "run_efficiency(\"HF-7B (sequential)\", hf_7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc935f-7ef3-4d18-bddd-833cc30f88d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
