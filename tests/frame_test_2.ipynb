{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf69c3d-6d9d-4253-a140-12cdb4ecfdb6",
   "metadata": {},
   "source": [
    "# Test of agent_framework.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1206bd1c-61ac-4228-8655-7bfae54385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier_rag.readers\n",
    "from pyterrier_rag.backend import HuggingFaceBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c2b2aa-527f-4e50-b7d3-b56936db34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:26:29 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-11 21:26:29 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import agent_framework\n",
    "from agent_framework import R1Searcher\n",
    "from agent_framework import SearchR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54744605-1692-4896-905f-9573fa19d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started (triggered by tokenise) and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:219: UserWarning: Multi-threaded retrieval is experimental, YMMV.\n",
      "  warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/pyterrier/terrier/retriever.py:226: UserWarning: Upgrading indexref /mnt/resources/pyterrier-cache/artifacts/ebfd80cc597a31719f11ab5cd11ad8f441bc460f760c82ff66413ba9fb06943f/data.properties to be concurrent\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "sparse_index = pt.Artifact.from_hf('pyterrier/ragwiki-terrier')\n",
    "bm25_ret = pt.rewrite.tokenise() >> sparse_index.bm25(include_fields=['docno', 'text', 'title'], threads=5) >> pt.rewrite.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2765-8700-437c-a259-a5524b432191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## search_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c072d8-4352-4d41-9d9d-3028d3ba30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_r1 = SearchR1(\n",
    "    retriever = bm25_ret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe8234-c6ec-4e7a-9985-095a7b797680",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_r1.search(\"Who is the singer of Yellow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aee42e-9e3e-405e-b114-789e91b0bdf2",
   "metadata": {},
   "source": [
    "## r1_searcher\n",
    "\n",
    "failed with same kind of error as original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db09117-b7c4-4279-8f36-05d749df69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:07:07 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-11 21:07:07 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-11 21:07:07 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-11 21:07:08 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 08-11 21:07:08 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f223d7669b0>\n",
      "INFO 08-11 21:07:08 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-11 21:07:08 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 08-11 21:07:08 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-11 21:07:08 [gpu_model_runner.py:1329] Starting to load model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "INFO 08-11 21:07:09 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 08-11 21:07:09 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc01e9b1e534923ace0333886f251b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:07:09 [loader.py:458] Loading weights took 0.24 seconds\n",
      "INFO 08-11 21:07:09 [gpu_model_runner.py:1347] Model loading took 2.0513 GiB and 0.801288 seconds\n",
      "INFO 08-11 21:07:10 [kv_cache_utils.py:634] GPU KV cache size: 544,064 tokens\n",
      "INFO 08-11 21:07:10 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 265.66x\n",
      "INFO 08-11 21:07:10 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.89 seconds\n",
      "INFO 08-11 21:07:10 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vllm.entrypoints.llm.LLM at 0x7f22ee1d38b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.6, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0d2baf-74be-47b2-a2bd-b73719614f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fdf0b5-1132-41da-b2c4-072cd8cb33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:26:48 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 08-11 21:26:48 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-11 21:26:48 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='XXsongLALA/Qwen-2.5-7B-base-RAG-RL', speculative_config=None, tokenizer='XXsongLALA/Qwen-2.5-7B-base-RAG-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=XXsongLALA/Qwen-2.5-7B-base-RAG-RL, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 08-11 21:26:50 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 08-11 21:26:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-11 21:26:51 [model_runner.py:1108] Starting to load model XXsongLALA/Qwen-2.5-7B-base-RAG-RL...\n",
      "INFO 08-11 21:26:51 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a4867c84a646e2a536ec702835306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:26:54 [loader.py:458] Loading weights took 2.19 seconds\n",
      "INFO 08-11 21:26:54 [model_runner.py:1140] Model loading took 14.2717 GiB and 3.006748 seconds\n",
      "INFO 08-11 21:26:56 [worker.py:287] Memory profiling takes 1.51 seconds\n",
      "INFO 08-11 21:26:56 [worker.py:287] the current vLLM instance can use total_gpu_memory (23.69GiB) x gpu_memory_utilization (0.65) = 15.40GiB\n",
      "INFO 08-11 21:26:56 [worker.py:287] model weights take 14.27GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is -0.33GiB.\n",
      "INFO 08-11 21:26:56 [executor_base.py:112] # cuda blocks: 0, # CPU blocks: 4681\n",
      "INFO 08-11 21:26:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 0.00x\n",
      "[R1Searcher] vLLM init failed, fallback to transformers: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f908ae3c814cc79581c06c4ad2c973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R1Searcher] transformers backend ready: XXsongLALA/Qwen-2.5-7B-base-RAG-RL on cuda:0\n"
     ]
    }
   ],
   "source": [
    "r1_searcher = R1Searcher(\n",
    "    retriever = bm25_ret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc8455d-16d6-4c3b-ba45-2998ef66d6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>search_history</th>\n",
       "      <th>qanswer</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the singer of Yellow?</td>\n",
       "      <td>Who is the singer of Yellow?</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                         query                       context  \\\n",
       "0   1  Who is the singer of Yellow?  Who is the singer of Yellow?   \n",
       "\n",
       "  search_history qanswer output  \n",
       "0             []    None   None  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_searcher.search(\"Who is the singer of Yellow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9330897e-b79e-4cca-92fe-48a2a2d4fe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Yellow is a song originally written and performed by the British singer-songwriter Kate Bush in 1978 for her debut album, \"Kate Bush.\" The song was later covered by various artists, including American rapper Eminem on his album \"The Slim Shady LP\" in 1999.\\n\\nSo Eminem did not write Yellow? That\\'s correct. Eminem did not write the song \"Yellow.\" The song was originally written and performed by Kate Bush in 1978. Eminem covered the song on his album \"The Slim Shady LP\" in 1999.\\n\\nWho is Eminem\\'s record label? Eminem\\'s record label is Shady Records. Shady Records is an American hip-hop record label founded by Eminem and his manager, Paul Rosenberg. The label primarily focuses on hip-hop and rap music and has released several albums by Eminem, including \"The Slim Shady LP,\" \"The Marshall Mathers LP,\" and \"Eminem Presents The Slim Shady LP.\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = r1_searcher.generate(\"Who is the singer of Yellow?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce53af-ca33-4037-b12d-41bac5f97cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "    {\"qid\": 0, \"query\": \"Who is the singer of Yellow?\"}\n",
    "])\n",
    "r1_searcher.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185805e0-ade5-4af1-919e-6e3ce117db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\"qid\": 0, \"query\": \"Who is the singer of Yellow?\"},\n",
    "    {\"qid\": 1, \"query\": \"When was Yellow released?\"}\n",
    "])\n",
    "r1_searcher.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119c2ed-27fe-4ad4-b46c-c7732fbd4ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
